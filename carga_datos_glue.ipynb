{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 1.0.8 \n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 2880 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 5\n",
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: glueetl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 5\n",
						"Idle Timeout: 2880\n",
						"Session ID: 2e0622f1-5b87-4669-8630-8c8fd6dadf7a\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.8\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session 2e0622f1-5b87-4669-8630-8c8fd6dadf7a to get into ready status...\n",
						"Session 2e0622f1-5b87-4669-8630-8c8fd6dadf7a has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"%idle_timeout 2880\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 5\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"+-------------------+----------------+--------+--------+--------+--------+------------------+\n",
						"|           datetime|          symbol|    open|    high|     low|   close|            volume|\n",
						"+-------------------+----------------+--------+--------+--------+--------+------------------+\n",
						"|2021-09-09 02:00:00|COINBASE:SHIBUSD|   1e-05|   9e-05|   6e-06|5.67e-05|2011405203216.0005|\n",
						"|2021-09-10 02:00:00|COINBASE:SHIBUSD|5.67e-05|   6e-05|6.76e-06|7.02e-06|3803666278890.0015|\n",
						"|2021-09-11 02:00:00|COINBASE:SHIBUSD|7.02e-06|7.35e-06|6.73e-06|6.77e-06|1663194647440.0005|\n",
						"|2021-09-12 02:00:00|COINBASE:SHIBUSD|6.78e-06|7.19e-06|6.64e-06|6.91e-06| 859389740690.0024|\n",
						"|2021-09-13 02:00:00|COINBASE:SHIBUSD|6.91e-06| 7.1e-06|6.17e-06|6.48e-06|  851872051559.002|\n",
						"|2021-09-14 02:00:00|COINBASE:SHIBUSD|6.48e-06| 6.9e-06|6.44e-06|6.67e-06|  799842914475.005|\n",
						"|2021-09-15 02:00:00|COINBASE:SHIBUSD|6.67e-06|6.91e-06| 6.6e-06|6.78e-06|1354279397992.0017|\n",
						"|2021-09-16 02:00:00|COINBASE:SHIBUSD|6.78e-06|1.34e-05|6.52e-06|8.81e-06|  33971466502853.0|\n",
						"|2021-09-17 02:00:00|COINBASE:SHIBUSD|8.81e-06| 9.7e-06| 8.1e-06|8.43e-06|  25611870030486.0|\n",
						"|2021-09-18 02:00:00|COINBASE:SHIBUSD|8.44e-06|8.62e-06|7.61e-06|7.86e-06| 8832750100865.002|\n",
						"+-------------------+----------------+--------+--------+--------+--------+------------------+\n",
						"only showing top 10 rows\n"
					]
				}
			],
			"source": [
				"from pyspark.sql.functions import col, substring\n",
				"\n",
				"# Definir el path de origen y destino en S3\n",
				"source_path = \"s3://cargadatostradingview/*/*/*.csv\"\n",
				"destination_path = \"s3://cargadatosplata/\"\n",
				"\n",
				"# Leer los archivos CSV desde S3 con inferencia de esquema optimizada\n",
				"df = spark.read.option(\"header\", \"true\").csv(source_path)\n",
				"df.show(10)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"ALMACENAMIENTO EN CAPA PLATA"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 21,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Proceso completado: Archivos CSV convertidos a Parquet y almacenados en S3 correctamente.\n"
					]
				}
			],
			"source": [
				"from pyspark.sql.functions import col, substring\n",
				"\n",
				"# Definir el path de origen y destino en S3\n",
				"source_path = \"s3://cargadatostradingview/*/*/*.csv\"\n",
				"destination_path = \"s3://cargadatosplata/\"\n",
				"\n",
				"# Leer los archivos CSV desde S3 con inferencia de esquema optimizada\n",
				"df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(source_path)\n",
				"\n",
				"# Extraer el año desde la columna \"datetime\"\n",
				"df = df.withColumn(\"year\", substring(col(\"datetime\"), 1, 4))\n",
				"\n",
				"df.write.mode(\"append\").partitionBy(\"symbol\", \"year\").option(\"basePath\", destination_path).parquet(destination_path)\n",
				"print(\"Proceso completado: Archivos CSV convertidos a Parquet y almacenados en S3 correctamente.\")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"ALMACENAMIENTO EN CAPA ORO"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Funciones para los KPIs"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"from pyspark.sql import SparkSession\n",
				"from pyspark.sql.functions import col, year, avg, when, lit\n",
				"from pyspark.sql.window import Window\n",
				"import pyspark.sql.functions as F\n",
				"\n",
				"\n",
				"# Función para calcular SMA\n",
				"def calculate_sma(df, period=14):\n",
				"    window_spec = Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\").rowsBetween(-period, 0)\n",
				"    return df.withColumn(f\"SMA_{period}\", F.avg(\"close\").over(window_spec))\n",
				"\n",
				"# Función corregida para EMA\n",
				"def calculate_ema(df, period=14):\n",
				"    alpha = 2 / (period + 1)\n",
				"    window_spec = Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\")\n",
				"\n",
				"    df = df.withColumn(\"prev_close\", F.lag(\"close\", 1).over(window_spec))\n",
				"    df = df.withColumn(f\"EMA_{period}\", (alpha * col(\"close\")) + ((1 - alpha) * col(\"prev_close\")))\n",
				"    \n",
				"    return df\n",
				"\n",
				"# Función para calcular RSI\n",
				"def calculate_rsi(df, period=14):\n",
				"    window_spec = Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\")\n",
				"\n",
				"    df = df.withColumn(\"change\", col(\"close\") - F.lag(\"close\", 1).over(window_spec))\n",
				"    df = df.withColumn(\"gain\", when(col(\"change\") > 0, col(\"change\")).otherwise(0))\n",
				"    df = df.withColumn(\"loss\", when(col(\"change\") < 0, -col(\"change\")).otherwise(0))\n",
				"\n",
				"    avg_gain = F.avg(\"gain\").over(Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\").rowsBetween(-period, 0))\n",
				"    avg_loss = F.avg(\"loss\").over(Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\").rowsBetween(-period, 0))\n",
				"\n",
				"    rsi = 100 - (100 / (1 + (avg_gain / avg_loss)))\n",
				"\n",
				"    return df.withColumn(\"RSI\", rsi)\n",
				"\n",
				"# Función para calcular MACD\n",
				"def calculate_macd(df, short_period=12, long_period=26, signal_period=9):\n",
				"    df = calculate_ema(df, short_period).withColumnRenamed(f\"EMA_{short_period}\", \"short_ema\")\n",
				"    df = calculate_ema(df, long_period).withColumnRenamed(f\"EMA_{long_period}\", \"long_ema\")\n",
				"\n",
				"    df = df.withColumn(\"MACD\", col(\"short_ema\") - col(\"long_ema\"))\n",
				"\n",
				"    window_spec = Window.partitionBy(\"symbol\", \"year\").orderBy(\"datetime\").rowsBetween(-signal_period, 0)\n",
				"    df = df.withColumn(\"Signal_Line\", F.avg(\"MACD\").over(window_spec))\n",
				"\n",
				"    return df\n",
				"\n",
				"# Leer datos desde S3"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"DataFrame[datetime: string, symbol: string, open: string, high: string, low: string, close: double, volume: string, year: int, SMA_14: double, prev_close: double, EMA_14: double, change: double, gain: double, loss: double, RSI: double, short_ema: double, long_ema: double, MACD: double, Signal_Line: double]\n"
					]
				}
			],
			"source": [
				"#Comprobar que funcione: # Extraer año de la columna datetime\n",
				"\n",
				"from pyspark.sql.functions import col, year  # Asegura que year está importado\n",
				"\n",
				"df = spark.read.option(\"header\", \"true\").csv(source_path)\n",
				"df = df.withColumn(\"year\", year(col(\"datetime\")))\n",
				"\n",
				"# Convertir close a tipo numérico (si está en string)\n",
				"df = df.withColumn(\"close\", col(\"close\").cast(\"double\"))\n",
				"\n",
				"# Aplicar los indicadores\n",
				"df = calculate_sma(df, period=14)\n",
				"df = calculate_ema(df, period=14)\n",
				"df = calculate_rsi(df, period=14)\n",
				"df = calculate_macd(df)\n",
				"df_selected = df.select(\n",
				"    \"datetime\",\n",
				"    \"symbol\",\n",
				"    \"SMA_14\",\n",
				"    \"EMA_14\",\n",
				"    \"RSI\",\n",
				"    \"MACD\",\n",
				"    \"Signal_Line\"\n",
				")\n",
				"\n",
				"# Mostrar los primeros 10 registros en formato tabular\n",
				"df_selected.show(10, truncate=False)\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 33,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"KPIs calculados y almacenados en S3 correctamente en la capa oro.\n"
					]
				}
			],
			"source": [
				"# Extraer año de la columna datetime\n",
				"\n",
				"from pyspark.sql.functions import col, year  # Asegura que year está importado\n",
				"\n",
				"df = spark.read.option(\"header\", \"true\").csv(source_path)\n",
				"df = df.withColumn(\"year\", year(col(\"datetime\")))\n",
				"\n",
				"# Convertir close a tipo numérico (si está en string)\n",
				"df = df.withColumn(\"close\", col(\"close\").cast(\"double\"))\n",
				"\n",
				"# Aplicar los indicadores\n",
				"df = calculate_sma(df, period=14)\n",
				"df = calculate_ema(df, period=14)\n",
				"df = calculate_rsi(df, period=14)\n",
				"df = calculate_macd(df)\n",
				"\n",
				"# Guardar en Parquet en la capa oro\n",
				"destination_gold_path = \"s3://cargadatosoro/\"\n",
				"df.write.mode(\"append\").partitionBy(\"symbol\", \"year\").parquet(destination_gold_path)\n",
				"\n",
				"print(\"KPIs calculados y almacenados en S3 correctamente en la capa oro.\")\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
